"use strict";(self.webpackChunkportfolio_docs=self.webpackChunkportfolio_docs||[]).push([[6053],{3420(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"ai-chag/ai-components","title":"AI Components Deep Dive","description":"CLIP Model - Core AI Technology","source":"@site/docs/ai-chag/ai-components.md","sourceDirName":"ai-chag","slug":"/ai-chag/ai-components","permalink":"/portfolio-docs/docs/ai-chag/ai-components","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/portfolio-docs/tree/main/docs/ai-chag/ai-components.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"aiChagSidebar","previous":{"title":"System Architecture","permalink":"/portfolio-docs/docs/ai-chag/architecture"},"next":{"title":"Technology Stack","permalink":"/portfolio-docs/docs/ai-chag/tech-stack"}}');var t=i(4848),s=i(8453);const a={sidebar_position:3},d="AI Components Deep Dive",l={},c=[{value:"CLIP Model - Core AI Technology",id:"clip-model---core-ai-technology",level:2},{value:"What is CLIP?",id:"what-is-clip",level:3},{value:"Why CLIP?",id:"why-clip",level:3},{value:"Technical Specifications",id:"technical-specifications",level:3},{value:"Indexing Pipeline",id:"indexing-pipeline",level:2},{value:"Flow Diagram",id:"flow-diagram",level:3},{value:"Code Implementation",id:"code-implementation",level:3},{value:"Retrieval Pipeline",id:"retrieval-pipeline",level:2},{value:"Flow Diagram",id:"flow-diagram-1",level:3},{value:"Code Implementation",id:"code-implementation-1",level:3},{value:"Vector Database (Qdrant)",id:"vector-database-qdrant",level:2},{value:"Why Vector Database?",id:"why-vector-database",level:3},{value:"HNSW Algorithm",id:"hnsw-algorithm",level:3},{value:"Data Structure",id:"data-structure",level:3},{value:"Sequence Diagram: End-to-End Query",id:"sequence-diagram-end-to-end-query",level:2},{value:"Performance Metrics",id:"performance-metrics",level:2},{value:"Next Steps",id:"next-steps",level:2}];function o(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"ai-components-deep-dive",children:"AI Components Deep Dive"})}),"\n",(0,t.jsx)(n.h2,{id:"clip-model---core-ai-technology",children:"CLIP Model - Core AI Technology"}),"\n",(0,t.jsx)(n.h3,{id:"what-is-clip",children:"What is CLIP?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CLIP"})," (Contrastive Language-Image Pre-training) is a multimodal AI model developed by OpenAI that understands both images and text in a shared embedding space."]}),"\n",(0,t.jsx)(n.mermaid,{value:'graph LR\n    subgraph "CLIP Architecture"\n        INPUT_IMG[Input Image] --\x3e IMG_ENC[Image Encoder Vision Transformer]\n        INPUT_TXT[Input Text] --\x3e TXT_ENC[Text Encoder Transformer]\n        \n        IMG_ENC --\x3e PROJ_IMG[Projection]\n        TXT_ENC --\x3e PROJ_TXT[Projection]\n        \n        PROJ_IMG --\x3e EMB_SPACE[Shared Embedding Space 512 dimensions]\n        PROJ_TXT --\x3e EMB_SPACE\n        \n        EMB_SPACE --\x3e SIM[Cosine Similarity]\n    end\n    \n    style EMB_SPACE fill:#ff6b6b,stroke:#c92a2a,stroke-width:3px,color:#fff\n    style SIM fill:#4dabf7,stroke:#1971c2,stroke-width:2px,color:#fff'}),"\n",(0,t.jsx)(n.h3,{id:"why-clip",children:"Why CLIP?"}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Multimodal"}),": Hi\u1ec3u c\u1ea3 h\xecnh \u1ea3nh v\xe0 v\u0103n b\u1ea3n",(0,t.jsx)(n.br,{}),"\n","\u2705 ",(0,t.jsx)(n.strong,{children:"Zero-shot learning"}),": Kh\xf4ng c\u1ea7n training l\u1ea1i",(0,t.jsx)(n.br,{}),"\n","\u2705 ",(0,t.jsx)(n.strong,{children:"Semantic understanding"}),": Hi\u1ec3u ng\u1eef ngh\u0129a, kh\xf4ng ch\u1ec9 pixel",(0,t.jsx)(n.br,{}),"\n","\u2705 ",(0,t.jsx)(n.strong,{children:"Pre-trained"}),": Trained tr\xean 400M image-text pairs"]}),"\n",(0,t.jsx)(n.h3,{id:"technical-specifications",children:"Technical Specifications"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Property"}),(0,t.jsx)(n.th,{children:"Value"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Model"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"sentence-transformers/clip-ViT-B-32"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Embedding Dimension"})}),(0,t.jsx)(n.td,{children:"512"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Input"})}),(0,t.jsx)(n.td,{children:"Images (RGB) + Text"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Output"})}),(0,t.jsx)(n.td,{children:"512-dim vectors"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Device"})}),(0,t.jsx)(n.td,{children:"GPU (CUDA) / CPU fallback"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"indexing-pipeline",children:"Indexing Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"flow-diagram",children:"Flow Diagram"}),"\n",(0,t.jsx)(n.mermaid,{value:"flowchart TD\n    START([Start Indexing]) --\x3e MODE{Ch\u1ecdn Mode}\n    \n    MODE --\x3e|Fresh Mode| DISCOVER[Discover Keyframes Qu\xe9t th\u01b0 m\u1ee5c]\n    MODE --\x3e|Precomputed Mode| LOAD[Load Embeddings T\u1eeb file]\n    \n    DISCOVER --\x3e FILTER[Apply Filter]\n    FILTER --\x3e METADATA[Create Metadata]\n    \n    METADATA --\x3e BATCH[Batch Processing 32 images/batch]\n    BATCH --\x3e IMG_LOAD[Load Images PIL Image.open]\n    \n    IMG_LOAD --\x3e CLIP_ENCODE[CLIP Encoding SentenceTransformer.encode]\n    CLIP_ENCODE --\x3e EMBEDDINGS[Embeddings 512-dim numpy arrays]\n    \n    LOAD --\x3e EMBEDDINGS\n    \n    EMBEDDINGS --\x3e SETUP[Setup Qdrant Collection]\n    SETUP --\x3e UPLOAD[Upload to Qdrant Points with vectors + metadata]\n    \n    UPLOAD --\x3e SUCCESS{Success?}\n    SUCCESS --\x3e|Yes| COMPLETE([\u2705 Indexing Complete])\n    SUCCESS --\x3e|No| ERROR([\u274c Failed])\n    \n    style CLIP_ENCODE fill:#ff6b6b,stroke:#c92a2a,stroke-width:3px,color:#fff\n    style EMBEDDINGS fill:#ff6b6b,stroke:#c92a2a,stroke-width:3px,color:#fff\n    style UPLOAD fill:#4dabf7,stroke:#1971c2,stroke-width:3px,color:#fff"}),"\n",(0,t.jsx)(n.h3,{id:"code-implementation",children:"Code Implementation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Generate Embeddings"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def generate_batch_embeddings(self, image_paths: List[str]) -> List[np.ndarray]:\n    batch_size = 32\n    embeddings = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_paths = image_paths[i:i + batch_size]\n        batch_images = [Image.open(path).convert('RGB') for path in batch_paths]\n        \n        # CLIP encoding\n        batch_embeddings = self.model.encode(batch_images, convert_to_numpy=True)\n        embeddings.extend(batch_embeddings)\n    \n    return embeddings\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Performance Optimization"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Batch processing: 32 images at once"}),"\n",(0,t.jsx)(n.li,{children:"GPU acceleration: ~100 images/sec"}),"\n",(0,t.jsx)(n.li,{children:"Error handling: Zero vectors for corrupted images"}),"\n",(0,t.jsx)(n.li,{children:"Progress tracking: tqdm progress bars"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"retrieval-pipeline",children:"Retrieval Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"flow-diagram-1",children:"Flow Diagram"}),"\n",(0,t.jsx)(n.mermaid,{value:"flowchart TD\n    START([User Query]) --\x3e API[API Request POST /qa/query]\n    \n    API --\x3e VALIDATE[Validate Input]\n    VALIDATE --\x3e TEXT_EMBED[Generate Text Embedding CLIP encode]\n    \n    TEXT_EMBED --\x3e QUERY_VEC[Query Vector 512-dim]\n    QUERY_VEC --\x3e CLEAN[Clean Filters]\n    \n    CLEAN --\x3e QDRANT[Qdrant Search Cosine Similarity]\n    QDRANT --\x3e RESULTS[Results with Scores]\n    \n    RESULTS --\x3e EXTRACT[Extract Info video_id, frame_idx]\n    EXTRACT --\x3e FORMAT[Format Response]\n    \n    FORMAT --\x3e OPTIONAL{Save CSV?}\n    OPTIONAL --\x3e|Yes| CSV[Save to CSV]\n    OPTIONAL --\x3e|No| RESPONSE\n    CSV --\x3e RESPONSE[JSON Response]\n    \n    RESPONSE --\x3e END([End])\n    \n    style TEXT_EMBED fill:#ff6b6b,stroke:#c92a2a,stroke-width:3px,color:#fff\n    style QUERY_VEC fill:#ff6b6b,stroke:#c92a2a,stroke-width:2px,color:#fff\n    style QDRANT fill:#4dabf7,stroke:#1971c2,stroke-width:3px,color:#fff"}),"\n",(0,t.jsx)(n.h3,{id:"code-implementation-1",children:"Code Implementation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Search Similar Vectors"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def search_similar(self, query_embedding: np.ndarray, limit: int = 10):\n    search_results = self.qdrant_client.search(\n        collection_name="video_keyframes",\n        query_vector=query_embedding,\n        limit=limit,\n        score_threshold=0.7  # Minimum similarity\n    )\n    \n    results = []\n    for result in search_results:\n        results.append({\n            "score": result.score,\n            "payload": result.payload\n        })\n    \n    return results\n'})}),"\n",(0,t.jsx)(n.h2,{id:"vector-database-qdrant",children:"Vector Database (Qdrant)"}),"\n",(0,t.jsx)(n.h3,{id:"why-vector-database",children:"Why Vector Database?"}),"\n",(0,t.jsx)(n.p,{children:"Traditional DB:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"SELECT * FROM videos WHERE title LIKE '%dog%'\n"})}),"\n",(0,t.jsx)(n.p,{children:"\u274c Can't understand semantics"}),"\n",(0,t.jsx)(n.p,{children:"Vector DB:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"search(query_vector=[0.23, -0.45, ...], top_k=10)\n"})}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Finds semantically similar content"}),"\n",(0,t.jsx)(n.h3,{id:"hnsw-algorithm",children:"HNSW Algorithm"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"HNSW"})," (Hierarchical Navigable Small World):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Graph-based ANN (Approximate Nearest Neighbor)"}),"\n",(0,t.jsx)(n.li,{children:"Multi-layer structure"}),"\n",(0,t.jsx)(n.li,{children:"Search complexity: O(log N)"}),"\n",(0,t.jsx)(n.li,{children:"Trade-off: 99% accuracy with 100x speed"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"data-structure",children:"Data Structure"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Qdrant Point"\n        ID[Point ID UUID]\n        VEC[Vector 512 floats]\n        PAY[Payload JSON metadata]\n    end\n    \n    ID --\x3e POINT[Point Object]\n    VEC --\x3e POINT\n    PAY --\x3e POINT\n    \n    POINT --\x3e COLL[(Collection video_keyframes)]\n    \n    style VEC fill:#ff6b6b,color:#fff\n    style COLL fill:#4dabf7,color:#fff'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Payload"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n  "keyframe_path": "/path/to/L30_V083/052.jpg",\n  "frame_idx": 52,\n  "video_id": "L30_V083"\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"sequence-diagram-end-to-end-query",children:"Sequence Diagram: End-to-End Query"}),"\n",(0,t.jsx)(n.mermaid,{value:'sequenceDiagram\n    autonumber\n    participant U as User\n    participant API as Retrieval API\n    participant CLIP as CLIP Model\n    participant Q as Qdrant\n    \n    U->>API: POST /qa/query {"query": "dog running"}\n    activate API\n    \n    API->>CLIP: encode("dog running")\n    activate CLIP\n    CLIP--\x3e>API: vector [0.23, -0.45, ...]\n    deactivate CLIP\n    \n    API->>Q: search_similar(vector, top_k=10)\n    activate Q\n    Q--\x3e>API: results with scores\n    deactivate Q\n    \n    API->>API: extract video_id & frame_idx\n    API--\x3e>U: JSON [{"video_id": "L30_V083", "frame_idx": 52, "score": 0.87}]\n    deactivate API'}),"\n",(0,t.jsx)(n.h2,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Metric"}),(0,t.jsx)(n.th,{children:"Value"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Indexing Speed"})}),(0,t.jsx)(n.td,{children:"~100 images/sec (GPU)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Search Latency"})}),(0,t.jsx)(n.td,{children:"<100ms"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Accuracy"})}),(0,t.jsx)(n.td,{children:"Top-10 recall ~85%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Embedding Dimension"})}),(0,t.jsx)(n.td,{children:"512"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Batch Size"})}),(0,t.jsx)(n.td,{children:"32 images"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"./tech-stack",children:"Technology Stack Details \u2192"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}}}]);